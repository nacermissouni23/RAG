{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Retrieval Pipeline\n",
    "\n",
    "This notebook replicates the functionality of `retrieval.ps1` for executing retrieval tasks.\n",
    "It performs document retrieval using BM25 or BGE-M3 retrievers and evaluates the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import custom modules\n",
    "from src.datasets.dataset import get_task_datasets\n",
    "from src.llms import Mock\n",
    "from src.tasks.retrieval import RetrievalTask\n",
    "from src.retrievers import CustomBM25Retriever, CustomBGEM3Retriever\n",
    "from src.embeddings.base import HuggingfaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters (matching retrieval.ps1)\n",
    "config = {\n",
    "    'ocr_type': 'gt',  # OCR type: 'gt', 'paddleocr', etc.\n",
    "    'retriever_type': 'bm25',  # Retriever type: 'bm25' or 'bge-m3'\n",
    "    'model_name': 'mock',\n",
    "    'retrieve_top_k': 2,\n",
    "    'data_path': 'data/qas_v2.json',\n",
    "    'docs_path': None,  # Will be set based on ocr_type\n",
    "    'task': 'Retrieval',\n",
    "    'evaluation_stage': 'retrieval',\n",
    "    'num_threads': 1,\n",
    "    'show_progress_bar': True,\n",
    "    'output_path': './output',\n",
    "    'chunk_size': 1024,\n",
    "    'chunk_overlap': 0\n",
    "}\n",
    "\n",
    "# Set docs_path based on ocr_type\n",
    "config['docs_path'] = f\"data/retrieval_base/{config['ocr_type']}\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed=0):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    import torch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(0)\n",
    "print(\"Random seed set to 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(f\"Loading dataset from {config['data_path']}...\")\n",
    "datasets = get_task_datasets(config['data_path'], config['task'])\n",
    "dataset = datasets[0]\n",
    "print(f\"Loaded {len(dataset)} data points\")\n",
    "\n",
    "# Display a sample data point\n",
    "if len(dataset) > 0:\n",
    "    print(\"\\nSample data point:\")\n",
    "    sample = dataset[0]\n",
    "    for key, value in sample.items():\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"  {key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Model and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mock LLM (not used in retrieval stage but required by the pipeline)\n",
    "llm = Mock()\n",
    "print(\"Initialized Mock LLM\")\n",
    "\n",
    "# Initialize the retriever based on configuration\n",
    "print(f\"\\nInitializing {config['retriever_type']} retriever...\")\n",
    "if config['retriever_type'] == \"bge-m3\":\n",
    "    embed_model = HuggingfaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "    retriever = CustomBGEM3Retriever(\n",
    "        config['docs_path'], \n",
    "        embed_model=embed_model, \n",
    "        embed_dim=1024,\n",
    "        chunk_size=config['chunk_size'], \n",
    "        chunk_overlap=config['chunk_overlap'], \n",
    "        similarity_top_k=config['retrieve_top_k']\n",
    "    )\n",
    "elif config['retriever_type'] == \"bm25\":\n",
    "    retriever = CustomBM25Retriever(\n",
    "        config['docs_path'], \n",
    "        chunk_size=config['chunk_size'], \n",
    "        chunk_overlap=config['chunk_overlap'], \n",
    "        similarity_top_k=config['retrieve_top_k']\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported retriever type: {config['retriever_type']}\")\n",
    "\n",
    "print(f\"Retriever initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Retrieval Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the retrieval task\n",
    "output_dir = os.path.join(config['output_path'], config['evaluation_stage'], config['ocr_type'])\n",
    "task = RetrievalTask(output_dir=output_dir)\n",
    "task.set_model(llm, retriever)\n",
    "print(f\"Retrieval task initialized with output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Execute Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each data point\n",
    "results = []\n",
    "\n",
    "print(f\"\\nProcessing {len(dataset)} data points...\")\n",
    "for data_point in tqdm(dataset, desc=\"Retrieving\", disable=not config['show_progress_bar']):\n",
    "    try:\n",
    "        # Perform retrieval\n",
    "        retrieval_results = task.retrieve_docs(data_point)\n",
    "        data_point[\"retrieval_results\"] = retrieval_results\n",
    "        \n",
    "        # Score the retrieval\n",
    "        result = {'id': data_point['ID'], **task.scoring(data_point)}\n",
    "        results.append(result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error processing data point {data_point.get('ID', 'unknown')}: {e}\")\n",
    "        data_point[\"retrieval_results\"] = []\n",
    "        result = {'id': data_point['ID'], **task.scoring(data_point)}\n",
    "        results.append(result)\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compute Overall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter valid results\n",
    "valid_results = [result for result in results if result['valid']]\n",
    "print(f\"Valid results: {len(valid_results)} out of {len(results)}\")\n",
    "\n",
    "# Compute overall metrics\n",
    "if len(valid_results) > 0:\n",
    "    overall = task.compute_overall(valid_results)\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    for key, value in overall.items():\n",
    "        print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "else:\n",
    "    overall = {}\n",
    "    print(\"No valid results to compute metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output\n",
    "info = {\n",
    "    'task': task.__class__.__name__, \n",
    "    'retriever': retriever.__class__.__name__,\n",
    "    'ocr_type': config['ocr_type'],\n",
    "    'retrieve_top_k': config['retrieve_top_k'],\n",
    "    'chunk_size': config['chunk_size'],\n",
    "    'chunk_overlap': config['chunk_overlap']\n",
    "}\n",
    "\n",
    "output = {\n",
    "    'info': info,\n",
    "    'overall': overall,\n",
    "    'results': results\n",
    "}\n",
    "\n",
    "# Define output path\n",
    "ret_name = {\n",
    "    \"CustomBM25Retriever\": \"bm25\",\n",
    "    \"CustomBGEM3Retriever\": \"bge-m3\"\n",
    "}[retriever.__class__.__name__]\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, f'all_{ret_name}_top{config[\"retrieve_top_k\"]}.json')\n",
    "\n",
    "# Save to JSON file\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\nResults saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Display Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for visualization\n",
    "results_df = []\n",
    "\n",
    "# Load QA data for additional context\n",
    "with open(config['data_path'], 'r', encoding='utf-8') as f:\n",
    "    qa_dict = {item['ID']: item for item in json.load(f)}\n",
    "\n",
    "for result in results:\n",
    "    if result['id'] in qa_dict:\n",
    "        qa_item = qa_dict[result['id']]\n",
    "        results_df.append({\n",
    "            'id': result['id'],\n",
    "            'ocr_type': config['ocr_type'],\n",
    "            'retriever': ret_name,\n",
    "            'domain': qa_item.get('doc_type', ''),\n",
    "            'doc_name': qa_item.get('doc_name', '').split('/')[-1],\n",
    "            'evidence_source': qa_item.get('evidence_source', ''),\n",
    "            'answer_form': qa_item.get('answer_form', ''),\n",
    "            'lcs': result['metrics']['lcs'],\n",
    "            'valid': result['valid']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results_df)\n",
    "print(f\"\\nResults DataFrame shape: {df.shape}\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Results Analysis by Evidence Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results by evidence source\n",
    "if len(df) > 0:\n",
    "    df['lcs_percent'] = df['lcs'] * 100\n",
    "    \n",
    "    # Group by evidence source\n",
    "    evidence_summary = df.groupby('evidence_source').agg({\n",
    "        'lcs_percent': ['mean', 'count'],\n",
    "        'valid': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nResults by Evidence Source:\")\n",
    "    display(evidence_summary)\n",
    "    \n",
    "    # Overall average\n",
    "    overall_avg = df['lcs_percent'].mean()\n",
    "    print(f\"\\nOverall Average LCS: {overall_avg:.2f}%\")\n",
    "    print(f\"Total Valid Results: {df['valid'].sum()} / {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results Analysis by Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results by domain\n",
    "if len(df) > 0:\n",
    "    domain_summary = df.groupby('domain').agg({\n",
    "        'lcs_percent': ['mean', 'count'],\n",
    "        'valid': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nResults by Domain:\")\n",
    "    display(domain_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Sample Retrieval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample retrieval results\n",
    "print(\"\\nSample Retrieval Results (Top 3):\")\n",
    "for i, result in enumerate(results[:3]):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Result {i+1} - ID: {result['id']}\")\n",
    "    print(f\"Valid: {result['valid']}\")\n",
    "    print(f\"LCS Score: {result['metrics']['lcs']:.4f}\")\n",
    "    \n",
    "    log = result['log']\n",
    "    print(f\"\\nQuestion: {log['quest']}\")\n",
    "    print(f\"\\nEvidence Source: {log['evidence_source']}\")\n",
    "    \n",
    "    if 'retrieval_context' in log and len(log['retrieval_context']) > 0:\n",
    "        print(f\"\\nRetrieved {len(log['retrieval_context'])} documents:\")\n",
    "        for j, doc in enumerate(log['retrieval_context']):\n",
    "            print(f\"\\n  Document {j+1}:\")\n",
    "            print(f\"    File: {doc.get('file_name', 'N/A')}\")\n",
    "            print(f\"    Page: {doc.get('page_idx', 'N/A')}\")\n",
    "            text = doc.get('text', '')\n",
    "            print(f\"    Text: {text[:200]}...\" if len(text) > 200 else f\"    Text: {text}\")\n",
    "    else:\n",
    "        print(\"\\nNo documents retrieved\")\n",
    "    \n",
    "    gt_context = log['ground_truth_context']\n",
    "    print(f\"\\nGround Truth Context: {gt_context[:200]}...\" if len(gt_context) > 200 else f\"\\nGround Truth Context: {gt_context}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
